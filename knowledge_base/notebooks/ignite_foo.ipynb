{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://faq.bsv.admin.ch/de/familienzulagen/wann-gilt-ein-jugendlicher-als-ausbildung'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract parsed HTML content from a web page. '''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_and_parse_html(url, tag):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content from a given URL and returns a BeautifulSoup object\n",
    "    for parsing the HTML.\n",
    "\n",
    "    :param url: The URL of the web page to fetch.\n",
    "    :return: A BeautifulSoup object representing the parsed HTML content.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Extract text from a web page.\"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        response.encoding = 'utf-8'\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "        return soup\n",
    "\n",
    "body = fetch_and_parse_html(URL, 'body')\n",
    "body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract language html tag value from a web page. '''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_language(url):\n",
    "    \"\"\"\n",
    "    Extracts the language from the HTML tag of a webpage.\n",
    "\n",
    "    :param url: The URL of the webpage to extract the language from.\n",
    "    :return: The language code (e.g., 'en', 'de') if found, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "            html_tag = soup.find('html')\n",
    "            return html_tag.get('lang') if html_tag and html_tag.get('lang') else None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "language = extract_language(URL)\n",
    "print(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract category from a web page. '''\n",
    "import urllib.parse\n",
    "\n",
    "def extract_category(url, category_position_in_path=2):\n",
    "    \"\"\"\n",
    "    Extracts a category segment from the URL of a webpage.\n",
    "\n",
    "    The category is determined by splitting the URL's path and selecting a segment\n",
    "    based on its position.\n",
    "\n",
    "    :param url: The URL to extract the category from.\n",
    "    :param category_position_in_path: The position of the segment in the URL path\n",
    "                                      that is considered the category. Default is 2.\n",
    "    :return: The extracted category segment if available, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        path_segments = parsed_url.path.strip('/').split('/')\n",
    "        if len(path_segments) >= category_position_in_path:\n",
    "            return path_segments[category_position_in_path - 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the URL: {e}\")\n",
    "    return None\n",
    "\n",
    "category = extract_category(URL)\n",
    "category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract QA pair from a web page. '''\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_and_clean_text(url, tags, remove_patterns=None):\n",
    "    \"\"\"\n",
    "    Extracts text from specified HTML tags of a web page and cleans it based on provided patterns.\n",
    "\n",
    "    :param url: The URL of the webpage to extract text from.\n",
    "    :param tags: A single tag or a list of tags to extract text from.\n",
    "    :param remove_patterns: A list of strings or regex patterns to remove from the extracted text.\n",
    "    :return: Extracted and cleaned text if available, otherwise an empty string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, features=\"html.parser\")\n",
    "            for script in soup(['header', 'footer']):\n",
    "                script.decompose()\n",
    "\n",
    "            if isinstance(tags, str):\n",
    "                tags = [tags]\n",
    "\n",
    "            text_parts = []\n",
    "            for tag in tags:\n",
    "                for element in soup.find_all(tag):\n",
    "                    text = element.get_text()\n",
    "                    lines = (line.strip() for line in text.splitlines())\n",
    "                    text_parts.append('\\n'.join(line for line in lines if line))\n",
    "\n",
    "            cleaned_text = '\\n\\n'.join(text_parts)\n",
    "            if remove_patterns:\n",
    "                for pattern in remove_patterns:\n",
    "                    cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "\n",
    "            return cleaned_text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "\n",
    "    return ''\n",
    "\n",
    "question = extract_and_clean_text(URL, ['h1'])\n",
    "\n",
    "remove_list = ['Antwort\\n', 'Rispondi\\n', 'Réponse\\n']\n",
    "answer = extract_and_clean_text(URL, ['article'], remove_list)\n",
    "\n",
    "print(f\"question: {question}\")\n",
    "print(f\"answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Iterate over a list of URLs in a sitemap.xml and\n",
    "    extract QA pair from a web page and save it to a SQLite database. \n",
    "    \n",
    "    @todo: Add a logic to update the existing records.\n",
    "'''\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_timestamp():\n",
    "    \"\"\" Gibt den aktuellen Zeitstempel zurück. \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def get_sitemap_urls(sitemap_url):\n",
    "    \"\"\" Extrahieren Sie URLs aus der Sitemap. \"\"\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url)\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        sitemap_xml = response.text\n",
    "        root = ET.fromstring(sitemap_xml)\n",
    "        namespace = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        urls = [url.find(\"sitemap:loc\", namespace).text for url in root]\n",
    "        return urls\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Fehler beim Abrufen der Sitemap: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_data_to_db(db_connection, data):\n",
    "    \"\"\" Speichern Sie die Daten in einer SQLite-Datenbank. \"\"\"\n",
    "    try:\n",
    "        cursor = db_connection.cursor()\n",
    "        cursor.execute(\"\"\"INSERT INTO faq_data (language, category, question, answer, source, created_at, updated_at)\n",
    "                          VALUES (?, ?, ?, ?, ?, ?, ?)\"\"\", data)\n",
    "        db_connection.commit()\n",
    "    except sqlite3.DatabaseError as e:\n",
    "        print(f\"Fehler beim Speichern in die Datenbank: {e}\")\n",
    "\n",
    "# SQLite Datenbank initialisieren\n",
    "conn = sqlite3.connect('/workspaces/b3rn_zero_copilot/vectorstors-container/vectorstors/data/bsv_faq.db')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS faq_data\n",
    "                 (id INTEGER PRIMARY KEY, language TEXT, category TEXT, question TEXT, answer TEXT, source TEXT, created_at TEXT, updated_at TEXT)''')\n",
    "conn.commit()\n",
    "\n",
    "remove_patterns = ['Antwort\\n', 'Rispondi\\n', 'Réponse\\n']\n",
    "sitemap_url = 'https://faq.bsv.admin.ch/sitemap.xml'\n",
    "urls = get_sitemap_urls(sitemap_url)\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        extracted_language = extract_language(url)\n",
    "        extracted_category = extract_category(url)\n",
    "        extracted_h1 = extract_and_clean_text(url, 'h1', remove_patterns)\n",
    "        extracted_article = extract_and_clean_text(url, 'article', remove_patterns)\n",
    "    \n",
    "        if extracted_h1 and extracted_language in ['de', 'it', 'fr', 'en']: \n",
    "            timestamp = get_current_timestamp()\n",
    "            data = (extracted_language, extracted_category, extracted_h1, extracted_article, url, timestamp, timestamp)\n",
    "            save_data_to_db(conn, data)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Verarbeitung der URL {url}: {e}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"Fertig! Alle Seiten wurden verarbeitet und in die SQLite-Datenbank gespeichert.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
